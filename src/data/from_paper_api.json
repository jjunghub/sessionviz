{"famous": [{"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "authors": "['cereb_142861', 'cereb_143229', 'cereb_143094', 'cereb_143230', 'cereb_143231', 'cereb_111755', 'cereb_143232', 'cereb_143095']", "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (\u2248 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", "publication": "MM   Proceedings of the  ACM Conference on Multimedia", "pub_year": 2014, "cereb_cite": 986, "max_cite": 3080, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 81}, {"year": 2016, "cite": 214}, {"year": 2017, "cite": 405}, {"year": 2018, "cite": 286}]}, {"title": "A survey on transfer learning", "authors": "['cereb_119133', 'cereb_143621']", "abstract": "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research. \u00a9 2006 IEEE.", "publication": "IEEE Transactions on Knowledge and Data Engineering", "pub_year": 2010, "cereb_cite": 382, "max_cite": 2710, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 1}, {"year": 2011, "cite": 11}, {"year": 2012, "cite": 16}, {"year": 2013, "cite": 27}, {"year": 2014, "cite": 26}, {"year": 2015, "cite": 53}, {"year": 2016, "cite": 72}, {"year": 2017, "cite": 94}, {"year": 2018, "cite": 82}]}, {"title": "LIBLINEAR: A library for large linear classification", "authors": "['cereb_142900', 'cereb_142901', 'cereb_142902', 'cereb_142903', 'cereb_137584']", "abstract": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.", "publication": "Journal of Machine Learning Research", "pub_year": 2008, "cereb_cite": 366, "max_cite": 3836, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 2}, {"year": 2010, "cite": 5}, {"year": 2011, "cite": 13}, {"year": 2012, "cite": 28}, {"year": 2013, "cite": 37}, {"year": 2014, "cite": 37}, {"year": 2015, "cite": 65}, {"year": 2016, "cite": 84}, {"year": 2017, "cite": 64}, {"year": 2018, "cite": 31}]}, {"title": "A tutorial on support vector regression", "authors": "['cereb_3849', 'cereb_142909']", "abstract": "In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.", "publication": "Statistics and Computing", "pub_year": 2004, "cereb_cite": 308, "max_cite": 3770, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 3}, {"year": 2007, "cite": 4}, {"year": 2008, "cite": 4}, {"year": 2009, "cite": 15}, {"year": 2010, "cite": 7}, {"year": 2011, "cite": 21}, {"year": 2012, "cite": 17}, {"year": 2013, "cite": 34}, {"year": 2014, "cite": 23}, {"year": 2015, "cite": 45}, {"year": 2016, "cite": 55}, {"year": 2017, "cite": 43}, {"year": 2018, "cite": 37}]}, {"title": "Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning", "authors": "['cereb_165030', 'cereb_50588', 'cereb_165031', 'cereb_165032', 'cereb_165033', 'cereb_165034', 'cereb_165035', 'cereb_27105', 'cereb_111394']", "abstract": "Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.", "publication": "IEEE Transactions on Medical Imaging", "pub_year": 2016, "cereb_cite": 282, "max_cite": 416, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 16}, {"year": 2017, "cite": 152}, {"year": 2018, "cite": 114}]}, {"title": "MatConvNet: Convolutional neural networks for MATLAB", "authors": "['cereb_153315', 'cereb_156189']", "abstract": "\u00a9 2015 ACM.MatConvNet is an open source implementation of Con-volutional Neural Networks (CNNs) with a deep integra-tion in the MATLAB environment. The toolbox is de-signed with an emphasis on simplicity and exibility. It exposes the building blocks of CNNs as easy-To-use MAT-LAB functions, providing routines for computing convolu-tions with -lter banks, feature pooling, normalisation, and much more. MatConvNet can be easily extended, often using only MATLAB code, allowing fast prototyping of new CNN architectures. At the same time, it supports efcient computation on CPU and GPU, allowing to train complex models on large datasets such as ImageNet ILSVRC con-taining millions of training examples.", "publication": "MM   Proceedings of the  ACM Multimedia Conference", "pub_year": 2015, "cereb_cite": 203, "max_cite": 699, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 2}, {"year": 2016, "cite": 30}, {"year": 2017, "cite": 99}, {"year": 2018, "cite": 72}]}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "authors": "['cereb_185416', 'cereb_147731', 'cereb_185417', 'cereb_185418', 'cereb_185419', 'cereb_170466', 'cereb_142087', 'cereb_185420', 'cereb_185421', 'cereb_185422', 'cereb_185423']", "abstract": "Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.", "publication": "Proceedings of the Annual International Symposium on Microarchitecture MICRO", "pub_year": 2014, "cereb_cite": 191, "max_cite": 183, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 1}, {"year": 2015, "cite": 15}, {"year": 2016, "cite": 47}, {"year": 2017, "cite": 81}, {"year": 2018, "cite": 47}]}, {"title": "A survey of techniques for internet traffic classification using machine learning", "authors": "['cereb_126797', 'cereb_46357']", "abstract": "The research community has begun looking for IP traffic classification techniques that do not rely on 'well known' TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed. \u00a9 2008 IEEE.", "publication": "IEEE Communications Surveys and Tutorials", "pub_year": 2008, "cereb_cite": 180, "max_cite": 636, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 8}, {"year": 2010, "cite": 8}, {"year": 2011, "cite": 14}, {"year": 2012, "cite": 10}, {"year": 2013, "cite": 20}, {"year": 2014, "cite": 12}, {"year": 2015, "cite": 25}, {"year": 2016, "cite": 30}, {"year": 2017, "cite": 29}, {"year": 2018, "cite": 24}]}, {"title": "ML-KNN: A lazy learning approach to multi-label learning", "authors": "['cereb_152231', 'cereb_145854']", "abstract": "Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms. \u00a9 2007 Pattern Recognition Society.", "publication": "Pattern Recognition", "pub_year": 2007, "cereb_cite": 163, "max_cite": 996, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 1}, {"year": 2009, "cite": 4}, {"year": 2010, "cite": 6}, {"year": 2011, "cite": 11}, {"year": 2012, "cite": 20}, {"year": 2013, "cite": 17}, {"year": 2014, "cite": 21}, {"year": 2015, "cite": 29}, {"year": 2016, "cite": 29}, {"year": 2017, "cite": 20}, {"year": 2018, "cite": 5}]}, {"title": "A systematic analysis of performance measures for classification tasks", "authors": "['cereb_82479', 'cereb_153463']", "abstract": "This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier's evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies. \u00a9 2009 Elsevier Ltd. All rights reserved.", "publication": "Information Processing and Management", "pub_year": 2009, "cereb_cite": 158, "max_cite": 864, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 3}, {"year": 2011, "cite": 4}, {"year": 2012, "cite": 1}, {"year": 2013, "cite": 7}, {"year": 2014, "cite": 18}, {"year": 2015, "cite": 23}, {"year": 2016, "cite": 32}, {"year": 2017, "cite": 41}, {"year": 2018, "cite": 29}]}], "trending": [{"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "authors": "['cereb_142861', 'cereb_143229', 'cereb_143094', 'cereb_143230', 'cereb_143231', 'cereb_111755', 'cereb_143232', 'cereb_143095']", "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (\u2248 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", "publication": "MM   Proceedings of the  ACM Conference on Multimedia", "pub_year": 2014, "cereb_cite": 986, "max_cite": 3080, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 81}, {"year": 2016, "cite": 214}, {"year": 2017, "cite": 405}, {"year": 2018, "cite": 286}]}, {"title": "Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning", "authors": "['cereb_165030', 'cereb_50588', 'cereb_165031', 'cereb_165032', 'cereb_165033', 'cereb_165034', 'cereb_165035', 'cereb_27105', 'cereb_111394']", "abstract": "Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.", "publication": "IEEE Transactions on Medical Imaging", "pub_year": 2016, "cereb_cite": 282, "max_cite": 416, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 16}, {"year": 2017, "cite": 152}, {"year": 2018, "cite": 114}]}, {"title": "MatConvNet: Convolutional neural networks for MATLAB", "authors": "['cereb_153315', 'cereb_156189']", "abstract": "\u00a9 2015 ACM.MatConvNet is an open source implementation of Con-volutional Neural Networks (CNNs) with a deep integra-tion in the MATLAB environment. The toolbox is de-signed with an emphasis on simplicity and exibility. It exposes the building blocks of CNNs as easy-To-use MAT-LAB functions, providing routines for computing convolu-tions with -lter banks, feature pooling, normalisation, and much more. MatConvNet can be easily extended, often using only MATLAB code, allowing fast prototyping of new CNN architectures. At the same time, it supports efcient computation on CPU and GPU, allowing to train complex models on large datasets such as ImageNet ILSVRC con-taining millions of training examples.", "publication": "MM   Proceedings of the  ACM Multimedia Conference", "pub_year": 2015, "cereb_cite": 203, "max_cite": 699, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 2}, {"year": 2016, "cite": 30}, {"year": 2017, "cite": 99}, {"year": 2018, "cite": 72}]}, {"title": "A survey on transfer learning", "authors": "['cereb_119133', 'cereb_143621']", "abstract": "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research. \u00a9 2006 IEEE.", "publication": "IEEE Transactions on Knowledge and Data Engineering", "pub_year": 2010, "cereb_cite": 382, "max_cite": 2710, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 1}, {"year": 2011, "cite": 11}, {"year": 2012, "cite": 16}, {"year": 2013, "cite": 27}, {"year": 2014, "cite": 26}, {"year": 2015, "cite": 53}, {"year": 2016, "cite": 72}, {"year": 2017, "cite": 94}, {"year": 2018, "cite": 82}]}, {"title": "Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images", "authors": "['cereb_190857', 'cereb_196695', 'cereb_67452']", "abstract": "Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks (CNNs), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the CNN feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant CNN (RICNN) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing CNN architectures. However, different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RICNN model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole RICNN network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.", "publication": "IEEE Transactions on Geoscience and Remote Sensing", "pub_year": 2016, "cereb_cite": 142, "max_cite": 134, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 3}, {"year": 2017, "cite": 78}, {"year": 2018, "cite": 61}]}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "authors": "['cereb_185416', 'cereb_147731', 'cereb_185417', 'cereb_185418', 'cereb_185419', 'cereb_170466', 'cereb_142087', 'cereb_185420', 'cereb_185421', 'cereb_185422', 'cereb_185423']", "abstract": "Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.", "publication": "Proceedings of the Annual International Symposium on Microarchitecture MICRO", "pub_year": 2014, "cereb_cite": 191, "max_cite": 183, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 1}, {"year": 2015, "cite": 15}, {"year": 2016, "cite": 47}, {"year": 2017, "cite": 81}, {"year": 2018, "cite": 47}]}, {"title": "LIBLINEAR: A library for large linear classification", "authors": "['cereb_142900', 'cereb_142901', 'cereb_142902', 'cereb_142903', 'cereb_137584']", "abstract": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.", "publication": "Journal of Machine Learning Research", "pub_year": 2008, "cereb_cite": 366, "max_cite": 3836, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 2}, {"year": 2010, "cite": 5}, {"year": 2011, "cite": 13}, {"year": 2012, "cite": 28}, {"year": 2013, "cite": 37}, {"year": 2014, "cite": 37}, {"year": 2015, "cite": 65}, {"year": 2016, "cite": 84}, {"year": 2017, "cite": 64}, {"year": 2018, "cite": 31}]}, {"title": "Big data deep learning: Challenges and perspectives", "authors": "['cereb_177673', 'cereb_186923']", "abstract": "Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.", "publication": "IEEE Access", "pub_year": 2014, "cereb_cite": 153, "max_cite": 174, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 1}, {"year": 2015, "cite": 12}, {"year": 2016, "cite": 40}, {"year": 2017, "cite": 60}, {"year": 2018, "cite": 40}]}, {"title": "Deep learning of binary hash codes for fast image retrieval", "authors": "['cereb_70608', 'cereb_202732', 'cereb_202733', 'cereb_192636']", "abstract": "Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. The utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.", "publication": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops", "pub_year": 2015, "cereb_cite": 89, "max_cite": 116, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 1}, {"year": 2016, "cite": 12}, {"year": 2017, "cite": 50}, {"year": 2018, "cite": 26}]}, {"title": "A tutorial on support vector regression", "authors": "['cereb_3849', 'cereb_142909']", "abstract": "In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.", "publication": "Statistics and Computing", "pub_year": 2004, "cereb_cite": 308, "max_cite": 3770, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 3}, {"year": 2007, "cite": 4}, {"year": 2008, "cite": 4}, {"year": 2009, "cite": 15}, {"year": 2010, "cite": 7}, {"year": 2011, "cite": 21}, {"year": 2012, "cite": 17}, {"year": 2013, "cite": 34}, {"year": 2014, "cite": 23}, {"year": 2015, "cite": 45}, {"year": 2016, "cite": 55}, {"year": 2017, "cite": 43}, {"year": 2018, "cite": 37}]}], "new": [{"title": "Neural network based predictive control of personalized heating systems", "authors": "['cereb_335777', 'cereb_183671', 'cereb_428673', 'cereb_220670']", "abstract": "\u00a9 2018 The Authors The aim of a personalized heating system is to provide a desirable microclimate for each individual when heating is needed. In this paper, we present a method based on machine learning algorithms for generation of predictive models for use in control of personalized heating systems. Data was collected from two individual test subjects in an experiment that consisted of 14 sessions per test subject with each session lasting 4 h. A dynamic recurrent nonlinear autoregressive neural network with exogenous inputs (NARX) was used for developing the models for the prediction of personalized heating settings. The models for subjects A and B were tested with the data that was not used in creating the neural network (unseen data) to evaluate the accuracy of the prediction. Trained NARX showed good performance when tested with the unseen data, with no sign of overfitting. For model A, the optimal network was with 12 hidden neurons with root mean square error equal to 0.043 and Pearson correlation coefficient equal to 0.994. The best result for model B was obtained with a neural network with 16 hidden neurons with root mean square error equal to 0.049 and Pearson correlation coefficient equal to 0.966. In addition to the neural network models, several other machine learning algorithms were tested. Furthermore, the models were on-line tested and the results showed that the test subjects were satisfied with the heating settings that were automatically controlled using the models. Tests with automatic control showed that both test subjects felt comfortable throughout the tests and test subjects expressed their satisfaction with the automatic control.", "publication": "Energy and Buildings", "pub_year": 2018, "cereb_cite": 8, "max_cite": 0, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 8}]}, {"title": "Privacy-preserving machine learning with multiple data providers", "authors": "['cereb_177738', 'cereb_250707', 'cereb_350507', 'cereb_142886', 'cereb_185672', 'cereb_173896']", "abstract": "\u00a9 2018 Elsevier B.V. With the fast development of cloud computing, more and more data storage and computation are moved from the local to the cloud, especially the applications of machine learning and data analytics. However, the cloud servers are run by a third party and cannot be fully trusted by users. As a result, how to perform privacy-preserving machine learning over cloud data from different data providers becomes a challenge. Therefore, in this paper, we propose a novel scheme that protects the data sets of different providers and the data sets of cloud. To protect the privacy requirement of different providers, we use public-key encryption with a double decryption algorithm (DD-PKE) to encrypt their data sets with different public keys. To protect the privacy of data sets on the cloud, we use \u03f5-differential privacy. Furthermore, the noises for the \u03f5-differential privacy are added by the cloud server, instead of data providers, for different data analytics. Our scheme is proven to be secure in the security model. The experiments also demonstrate the efficiency of our protocol with different classical machine learning algorithms.", "publication": "Future Generation Computer Systems", "pub_year": 2018, "cereb_cite": 7, "max_cite": 3, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 7}]}, {"title": "Comparative Study of Hybrid Artificial Intelligence Approaches for Predicting Hangingwall Stability", "authors": "['cereb_310512', 'cereb_310513', 'cereb_350509', 'cereb_350508', 'cereb_350510']", "abstract": "\u00a9 2017 American Society of Civil Engineers.Five hybrid artificial intelligence (AI) approaches based on machine learning (ML) and metaheuristic algorithms were proposed to predict open stope hangingwall (HW) stability. The ML algorithms consisted of logistic regression (LR), multilayer perceptron neural networks (MLPNN), decision tree (DT), gradient boosting machine (GBM), and support vector machine (SVM), and the firefly algorithm (FA) was used to tune their hyperparameters. The objectives are to compare different hybrid AI approaches for HW stability prediction and investigate the relative importance of its influencing variables. Performance measures were chosen to be the confusion matrix, the receiver operating characteristic (ROC) curve, and the area under the ROC curve (AUC). The results showed that the proposed hybrid AI approaches had great potential to predict HW stability and the FA was efficient in ML hyperparameters tuning. The AUC values of the optimum GBM, SVM, and LR models on the testing set were 0.855, 0.816, and 0.801, respectively, denoting that their performance was excellent. The optimum GBM model with the top left cutoff or the Youden's cutoff was recommended for HW prediction in terms of the accuracy, the true positive rate and the AUC value. The relative importance of influencing variables on HW stability was obtained, in which stope design method was found to be the most significant variable.", "publication": "Journal of Computing in Civil Engineering", "pub_year": 2018, "cereb_cite": 7, "max_cite": 9, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 7}]}, {"title": "Automatic histologically-closer classification of skin lesions", "authors": "['cereb_101277', 'cereb_119705', 'cereb_106558', 'cereb_392013', 'cereb_3425', 'cereb_11375', 'cereb_130551']", "abstract": "\u00a9 2018 Elsevier Ltd According to the American Cancer Society, melanoma is one of the most common types of cancer in the world. In 2017, approximately 87,110 new cases of skin cancer were diagnosed in the United States alone. A dermatoscope is a tool that captures lesion images with high resolution and is one of the main clinical tools to diagnose, evaluate and monitor this disease. This paper presents a new approach to classify melanoma automatically using structural co-occurrence matrix (SCM) of main frequencies extracted from dermoscopy images. The main advantage of this approach consists in transform the SCM in an adaptive feature extractor improving his power of discrimination using only the image as parameter. The images were collected from the International Skin Imaging Collaboration (ISIC) 2016, 2017 and Pedro Hispano Hospital (PH2) datasets. Specificity (Spe), sensitivity (Sen), positive predictive value, F Score, Harmonic Mean, accuracy (Acc) and area under the curve (AUC) were used to verify the efficiency of the SCM. The results show that the SCM in the frequency domain work automatically, where it obtained better results in comparison with local binary patterns, gray-level co-occurrence matrix and invariant moments of Hu as well as compared with recent works with the same datasets. The results of the proposed approach were: Spe 95.23%, 92.15% and 99.4%, Sen 94.57%, 89.9% and 99.2%, Acc 94.5%, 89.93% and 99%, and AUC 92%, 90% and 99% in ISIC 2016, 2017 and PH2 datasets, respectively.", "publication": "Computerized Medical Imaging and Graphics", "pub_year": 2018, "cereb_cite": 6, "max_cite": 0, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 6}]}, {"title": "Optimized cuttlefish algorithm for diagnosis of Parkinson's disease", "authors": "['cereb_318150', 'cereb_449769', 'cereb_449770', 'cereb_449771', 'cereb_431743', 'cereb_501693', 'cereb_130551']", "abstract": "\u00a9 2018 Elsevier B.V. This paper presents an optimized cuttlefish algorithm for feature selection based on the traditional cuttlefish algorithm, which can be used for diagnosis of Parkinson's disease at its early stage. Parkinson is a central nervous system disorder, caused due to the loss of brain cells. Parkinson's disease is incurable and could eventually lead to death but medications can help to control symptoms and elongate the patient's life to some extent. The proposed model uses the traditional cuttlefish algorithm as a search strategy to ascertain the optimal subset of features. The decision tree and k-nearest neighbor classifier as a judgment on the selected features. The Parkinson speech with multiple types of sound recordings and Parkinson Handwriting sample's datasets are used to evaluate the proposed model. The proposed algorithm can be used in predicting the Parkinson's disease with an accuracy of approximately 94% and help individual to have proper treatment at early stage. The experimental result reveals that the proposed bio-inspired algorithm finds an optimal subset of features, maximizing the accuracy, minimizing number of features selected and is more stable.", "publication": "Cognitive Systems Research", "pub_year": 2018, "cereb_cite": 6, "max_cite": 0, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 6}]}, {"title": "An efficient binary Salp Swarm Algorithm with crossover scheme for feature selection problems", "authors": "['cereb_291446', 'cereb_79727', 'cereb_4608', 'cereb_282001', 'cereb_2814', 'cereb_154015', 'cereb_240546']", "abstract": "\u00a9 2018 Elsevier B.V.Searching for the (near) optimal subset of features is a challenging problem in the process of feature selection (FS). In the literature, Swarm Intelligence (SI) algorithms show superior performance in solving this problem. This motivated our attempts to test the performance of the newly proposed Salp Swarm Algorithm (SSA) in this area. As such, two new wrapper FS approaches that use SSA as the search strategy are proposed. In the first approach, eight transfer functions are employed to convert the continuous version of SSA to binary. In the second approach, the crossover operator is used in addition to the transfer functions to replace the average operator and enhance the exploratory behavior of the algorithm. The proposed approaches are benchmarked on 22 well-known UCI datasets and the results are compared with 5 FS methods: Binary Grey Wolf Optimizer (BGWO), Binary Gravitational Search Algorithms (BGSA), Binary Bat Algorithm (BBA), Binary Particle Swarm Optimization (BPSO), and Genetic Algorithm (GA). The paper also considers an extensive study of the parameter setting for the proposed technique. From the results, it is observed that the proposed approach significantly outperforms others on around 90% of the datasets.", "publication": "KnowledgeBased Systems", "pub_year": 2018, "cereb_cite": 6, "max_cite": 0, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 6}]}, {"title": "A Survey of Data Mining and Deep Learning in Bioinformatics", "authors": "['cereb_405715', 'cereb_450079', 'cereb_118931', 'cereb_368143', 'cereb_69894', 'cereb_277924']", "abstract": "The fields of medicine science and health informatics have made great progress recently and have led to in-depth analytics that is demanded by generation, collection and accumulation of massive data. Meanwhile, we are entering a new period where novel technologies are starting to analyze and explore knowledge from tremendous amount of data, bringing limitless potential for information growth. One fact that cannot be ignored is that the techniques of machine learning and deep learning applications play a more significant role in the success of bioinformatics exploration from biological data point of view, and a linkage is emphasized and established to bridge these two data analytics techniques and bioinformatics in both industry and academia. This survey concentrates on the review of recent researches using data mining and deep learning approaches for analyzing the specific domain knowledge of bioinformatics. The authors give a brief but pithy summarization of numerous data mining algorithms used for preprocessing, classification and clustering as well as various optimized neural network architectures in deep learning methods, and their advantages and disadvantages in the practical applications are also discussed and compared in terms of their industrial usage. It is believed that in this review paper, valuable insights are provided for those who are dedicated to start using data analytics methods in bioinformatics.", "publication": "Journal of Medical Systems", "pub_year": 2018, "cereb_cite": 6, "max_cite": 0, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 6}]}, {"title": "Matminer: An open source toolkit for materials data mining", "authors": "['cereb_77168', 'cereb_449484', 'cereb_361588', 'cereb_96722', 'cereb_417622', 'cereb_183989', 'cereb_65246', 'cereb_178761', 'cereb_449485', 'cereb_449486', 'cereb_306821', 'cereb_179682', 'cereb_72646', 'cereb_414244', 'cereb_40877', 'cereb_154105']", "abstract": "\u00a9 2018 Elsevier B.V. As materials data sets grow in size and scope, the role of data mining and statistical learning methods to analyze these materials data sets and build predictive models is becoming more important. This manuscript introduces matminer, an open-source, Python-based software platform to facilitate data-driven methods of analyzing and predicting materials properties. Matminer provides modules for retrieving large data sets from external databases such as the Materials Project, Citrination, Materials Data Facility, and Materials Platform for Data Science. It also provides implementations for an extensive library of feature extraction routines developed by the materials community, with 47 featurization classes that can generate thousands of individual descriptors and combine them into mathematical functions. Finally, matminer provides a visualization module for producing interactive, shareable plots. These functions are designed in a way that integrates closely with machine learning and data analysis packages already developed and in use by the Python data science community. We explain the structure and logic of matminer, provide a description of its various modules, and showcase several examples of how matminer can be used to collect data, reproduce data mining studies reported in the literature, and test new methodologies.", "publication": "Computational Materials Science", "pub_year": 2018, "cereb_cite": 5, "max_cite": 0, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 5}]}, {"title": "Accelerated search for perovskite materials with higher Curie temperature based on the machine learning methods", "authors": "['cereb_399762', 'cereb_449472', 'cereb_205350']", "abstract": "\u00a9 2018 Elsevier B.V. Curie temperature (Tc), the second order phase transition temperature, is also one of the important physical properties of perovskite materials. It is a meaningful work to quickly and efficiently predict Tc of new perovskite materials before doing a considerable amount of experimental work. In the work, SVM (support vector machine), RVM (relevance vector machine) and RF (random forest) were employed to establish the prediction models of Tc with the physicochemical parameters, respectively. The results reveal that the three models all have high precision and reliability. According to K-fold cross validation, the SVR model had better prediction performance than the RVM and RF models. Meanwhile, the potential perovskite material with higher Tc was found by using the SVR model integrated with the search strategy of genetic algorithm from the virtual samples. The methods outlined here can provide valuable hints into the exploration of materials with desired property and can accelerate the process of materials design.", "publication": "Computational Materials Science", "pub_year": 2018, "cereb_cite": 5, "max_cite": 0, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 5}]}, {"title": "A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction", "authors": "['cereb_355258', 'cereb_156514']", "abstract": "Deep convolutional neural networks (DCNNs) have led to breakthrough results in numerous practical machine learning tasks, such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a classifier. The mathematical analysis of DCNNs for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory that encompasses general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned filters), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating, e.g., sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor, we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth, and we establish deformation sensitivity bounds that apply to signal classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz functions.", "publication": "IEEE Transactions on Information Theory", "pub_year": 2018, "cereb_cite": 5, "max_cite": 5, "citeburst": [{"year": 1969, "cite": 0}, {"year": 1970, "cite": 0}, {"year": 1971, "cite": 0}, {"year": 1972, "cite": 0}, {"year": 1973, "cite": 0}, {"year": 1974, "cite": 0}, {"year": 1975, "cite": 0}, {"year": 1976, "cite": 0}, {"year": 1977, "cite": 0}, {"year": 1978, "cite": 0}, {"year": 1979, "cite": 0}, {"year": 1980, "cite": 0}, {"year": 1981, "cite": 0}, {"year": 1982, "cite": 0}, {"year": 1983, "cite": 0}, {"year": 1984, "cite": 0}, {"year": 1985, "cite": 0}, {"year": 1986, "cite": 0}, {"year": 1987, "cite": 0}, {"year": 1988, "cite": 0}, {"year": 1989, "cite": 0}, {"year": 1990, "cite": 0}, {"year": 1991, "cite": 0}, {"year": 1992, "cite": 0}, {"year": 1993, "cite": 0}, {"year": 1994, "cite": 0}, {"year": 1995, "cite": 0}, {"year": 1996, "cite": 0}, {"year": 1997, "cite": 0}, {"year": 1998, "cite": 0}, {"year": 1999, "cite": 0}, {"year": 2000, "cite": 0}, {"year": 2001, "cite": 0}, {"year": 2002, "cite": 0}, {"year": 2003, "cite": 0}, {"year": 2004, "cite": 0}, {"year": 2005, "cite": 0}, {"year": 2006, "cite": 0}, {"year": 2007, "cite": 0}, {"year": 2008, "cite": 0}, {"year": 2009, "cite": 0}, {"year": 2010, "cite": 0}, {"year": 2011, "cite": 0}, {"year": 2012, "cite": 0}, {"year": 2013, "cite": 0}, {"year": 2014, "cite": 0}, {"year": 2015, "cite": 0}, {"year": 2016, "cite": 0}, {"year": 2017, "cite": 0}, {"year": 2018, "cite": 5}]}]}